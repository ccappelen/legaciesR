---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# legaciesr

<!-- badges: start -->

<!-- [![Codecov test coverage](https://codecov.io/gh/ccappelen/legaciesR/graph/badge.svg)](https://app.codecov.io/gh/ccappelen/legaciesR) -->
<!-- [![R-CMD-check](https://github.com/ccappelen/legaciesR/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/ccappelen/legaciesR/actions/workflows/R-CMD-check.yaml) -->
<!-- badges: end -->

The `legaciesr` package provides a set of functions used to aggregate and summarize the mapping and ID data collected in the [LEGACIES](https://www.legacies-project.com) project. It allows users to, i.a., (1) create contour polygons capturing the territorial extent of historical states at different probability thresholds, (2) create a grid with various summary measures of historical state presence, and (3) add a range of commonly used covariates to the polygon and grid data.

## Installation

You can install the development version of legaciesr from [GitHub](https://github.com/) with:

``` {r install, eval = FALSE}
install.packages("devtools")
devtools::install_github("ccappelen/legaciesR")
```


```{r packages, eval = TRUE, message = FALSE}
library(legaciesr)
library(sf) 
library(dplyr)
library(ggplot2)
```

The plots produced below rely on custom functions available in my personal package which can be loaded with the following code: 
```{r cappelen pkg, eval = FALSE, message = FALSE}
devtools::install_github("ccappelen/cappelenR")
library(cappelenR)
```

## Package overview and workflow

The legaciesr package is composed of a number of functions designed to process and match the raw mapping and state data collected in the LEGACIES project. While they can, in most cases, be run independently of each other, they are mostly meant to be used in the workflow described in the remainder of this README. In short, it consists of 
<!-- (1) fixing invalid geometries in the map data, (2) preprocess the map data and match with information in the ISD state data, (3) detect potential errors in the data set, (4) generate contour polygons from the raw map data, (5) generate a grid cell data set with various summary measures of historical statehood, and (6) match the grid cell data with numerous other data sources commonly used in empirical applications.  -->

1. Detecting and fixing invalid geometries in the map data. 
2. Preprocessing the map data and match with information in the ISD state data. 
3. Detecting potential errors in the data set. 
4. Generating contour polygons from the raw map data. (Optional)
5. Generating a grid cell data set with various summary measures of historical statehood. 
6. Matching the grid cell data with numerous other data sources commonly used in empirical applications. 

Each function (corresponding to each of the six steps) is further documented in their respective help documentation. This guide will outline the basic features of the functions and their intended usage. 


## Support for parallel processing:

Some functions enable parallel processing to speed up intensive tasks. Parallel processing is is implemented using the [future::future] framework. There are two ways of running jobs in parallel: `multicore` which uses 'forking' to run multiple jobs in parallel with shared memory and `multisession` which launches a set of background R sessions. 'Forking' can be faster than multisession because of the larger overhead associated with copying the active environment to each background R session (whereas forking processes shares memory). However, 'forking' is not supported on Windows platforms and is considered unstable when running from within RStudio (on both Windows and Unix systems such as MacOS). The function will automatically determine whether `multicore` is supported by the platform and choose the appropriate plan.

The greater overhead associated with `multisession` is primarily during the first parallel run in a given R session (since the background R sessions stays available for additional parallel jobs). It is possible to define a [future::plan("multisession")] in the global environment, which will minimize overhead in subsequent parallel jobs (apart from the first). The function will automatically detect if a `multisession` plan has been set globally and, thus, will not close background sessions after running.

It is therefore recommended to start the script by setting a [future::plan]: 

```{r Initialize parallel processing, eval = TRUE, message = FALSE}
future::plan("multisession", workers = future::availableCores())
## The above code sets up parallel processing on all available cores. 
## This can be changed with the 'workers' option. 
```


## Reading the data

The raw map and ISD data is not currently included in the package and therefore has to be loaded from the user's own directory: 

```{r Set data paths, eval = FALSE}
shp_folder <- "path to map data folder"
shp_name <- "name of shapefile"
isd_path <- "file path to isd data"
```


```{r Read data, eval = FALSE}
shp <- st_read(shp_path, shp_name)
rm(shp_folder, shp_name)

isd <- readxl::read_xlsx(isd_path)
rm(isd_path)
```

```{r read data hidden, eval = TRUE, echo = FALSE, message = FALSE, warning = FALSE}
shp <- read_sf("data_private", "master_shapefile")
isd <- readxl::read_xlsx("data_private/legacies_polity_years_reduced.xlsx")
```

## Invalid geometries

Some maps in the raw data may be "invalid" which will result in errors in many spatial data analyses. This can happen for all kinds of reasons; typically it is the result of crossing boundaries. `fix_invalid()` attempts to detect and fix these invalid geometries. It is a wrapper around the `sf::st_make_valid()` function that attempts to reiteratively lower the allowed precision to rebuild valid geometries. The function will return the same data set with fixed geometries (if it was able to fix them). The returned data set will include three columns describing the status of the geometry, e.g., whether it was invalid and whether it was successfully fixed. It will also (by default) print a summary of how many geometries were invalid, how many were fixed, and how many were unsuccessful. 

Because invalid geometries can happen for many reasons that may point to other data errors as well, it is recommended to check for valid geometries and run the `legaciesr::fix_invalid()` to identify potential issues.


```{r fix invalid geometries, eval = TRUE, warning = FALSE}
shp <- fix_invalid(shp)

## CROP SHP FOR NOW
shp <- shp[st_within(shp,
                     rnaturalearthdata::countries50 |>
                       filter(subregion == "Western Africa") |>
                       st_union(), sparse = F),]
```


## Preprocessing

The next step is to prepare the map data to be in a suitable format for creating grid data, matching with data from the ISD data, etc. It is therefore also necessary to provide the ISD data in order to match the two data sets. By default, the function will (1) expand the data to have one row per year for maps that are assigned a range of years, (2) fix potential issues such as three-digit years due to approximate dating, (3) exclude maps without your assignment, (4) add information on capitals (names and coordinates), (5) crop all geometries to coastlines\*, (6) exclude maps marked as "incomplete", (7) exclude maps of core regions (if they are marked as such)\*\*, and (8) exclude maps for years in which a state is not considered sovereign (in the ISD data)\*\*\*. Each of these options can be disabled. It is also possible to exclude maps based on the hierarchy coding of states, i.e., if states are coded as tributary and/or dependency. 

\* The cropped geometries currently result in errors when creating contour polygons (but work for grid data)
\*\* There are currently errors in the coding of core regions. 
\*\*\* By default, maps are included if they fall within a 5-year window of when a state is considered sovereign. The size of the window can be changed with the `margin_sovereign` option. 

```{r preprocessing, eval = TRUE, message = FALSE}
shp <- prepare_shapes(shp = shp, state_data = isd,
                      id_var = COWID, period_var = year,
                      range_min = lyear, range_max = hyear,
                      crop_to_land = FALSE, ## 'get_contours' currently not working when cropped to land
                      exclude_core = FALSE ## Currently errors in 'core' and 'Core.Great' coding.
                      )
```


## Detecing errors

To identify potential errors in the map data, the function `detect_errors` checks for various issues in the data set, such as the existence of duplicate COWIDs, missing COWIDs, errors in year assignment (e.g., outside study range), if there are geometries not overlapping with other geometries of the same COWID, and if there are capitals falling outside the geometry of a given COWID. All these CAN indicate errors, but it does not mean that it is necessarily an error. The function should therefore be used primarily as an easy way to identify potential issues with the map data that should be investigated further. 

The function returns various objects for easy identification of errors. First, it prints a report detailing whether and how many potential issues there are for a given type of error. 

```{r error detection, eval = TRUE, message = TRUE}
errors <- detect_errors(shp = shp, capital_data = isd,
                        id_var = COWID, period_var = year,
                        progress = FALSE)
```

Second, it returns a list composed of (1) the original data with columns indicating potential issues for each error type and (2) separate subsets of the data set including only observations identified as a potential issue for each of the error types. For instance, the code below returns only those COWIDs for which there are geometries not overlapping with other geometries of the same COWID (which might indicate either erroneous COWID assignment or erroneous geocoding of the map). 

```{r error report, eval = TRUE, message = TRUE}
shp_non_overlap <- errors$report$non_overlap
shp_non_overlap
```


## Create contour polygons

Contour polygons capture the varying degrees to which a given area is covered by the digitized maps. The contours divide the region covered by the union of all maps into custom intervals of the share of maps covering a given area. By default, the function divides the territory into four contours (correponding to 0-1, 0.25-1, 0.5-5, and 0.75-1). But the number of percentiles can be specified with the `cuts` option (see documentation for details). 

```{r create contours, eval = TRUE}
df_contour <- get_contours(shp, id_var = COWID)
``` 

The code below plots the contour polygons for the Sokoto Caliphate: 

```{r plot contours, eval = TRUE}
# Load map of Africa
afr <- rnaturalearthdata::countries50 |>
  filter(continent == "Africa")

# Plot contour polygons 
df_contour |>
  filter(COWID == "SOK") |>
  ggplot() +
  geom_sf(data = afr, fill = "grey90", color = "grey20") +
  geom_sf(aes(fill = label), color = NA) +
  scale_fill_viridis_d(option = "plasma", direction = 1, na.value = "NA") +
  labs(fill = "Share of polygons") +
  cappelenR::my_maptheme() +
  cappelenR::coord_bbox(df_contour |> filter(COWID == "SOK"),
                        expand_x = 6, expand_y = 3) +
  theme(legend.position = "bottom", legend.justification = "center",
        legend.title.position = "top") 
```

By default, the function summarizes the maps across the entire period (i.e. 1750-1920). However, it is also possible to specify the contour polygons by drawn separately by specified period (see documentation for further details). 

```{r contour panel, eval = TRUE}
df_contour_panel <- get_contours(shp, id_var = COWID, by_period = TRUE, period_var = year)

df_contour_panel |>
  filter(COWID == "SOK") |>
  ggplot() +
  geom_sf(data = afr, fill = "grey90", color = "grey20") +
  geom_sf(aes(fill = label), color = NA) +
  scale_fill_viridis_d(option = "plasma", direction = 1, na.value = "NA") +
  labs(fill = "Share of maps") +
  cappelenR::my_maptheme() +
  cappelenR::coord_bbox(df_contour |> filter(COWID == "SOK"),
                        expand_x = 6, expand_y = 3) +
  theme(legend.position = "bottom", legend.justification = "center",
        legend.title.position = "top") +
  facet_wrap(~ period)
```


## Create grid data

Finally, `get_grid` allows you to create a grid cell data set with various summary measures of how many maps cover a particular area. By default, the function creates a grid covering the entire extent of the provided map data set, but it is also possible to specify a particular area by providing a raster object with that extent. The function calculates several different operationalizations of the overall idea of capturing to what extent a given area was controlled by a state. The different measures rely on different rules for specyfing what state to use for summarizing the share of maps if there are more than one state covering a particular grid cell. 

1. _polysh_largest_count_: The share of all polygons in a grid cell for the state with the largest total number of polygons. 
2. _polysh_largest_area_: The share of all polygons in a grid cell for the state with the largest single polygon (NB: This could be operataionalized differently as based on the area of the union of polygons for each state or the median area of polygons for each state). 
3. _polysh_largest_share_: The share of all polygons in a grid cell for the state with the largest share in that grid cell. 
4. _polysh_across_: The average share of polygons across all states intersecting a given grid cell. 

In addition to these, there are two measures capturing slightly different concepts related to borders and contested territory. 

5. _bordersh_: The share of polygons with a border intersecting a given grid cell, relative to all polygons intersecting the grid cell (across all states). A higher share indicates that the area is more likely to be a border region. 
6. _contested_: A measure capturing the idea of contested territory. Is is calculated as negative sum of state-specific shares of polygons intersecting a grid cell weighted by the logarithm of the same share: \eqn{E = -\sum{}p*ln(p)}, where \eqn{p} is the state-specific share of polygons intersecting a grid cell. (NB: This measure is still in development and the exact equation might change.) 

```{r get grid, eval = TRUE, message = FALSE}
df_grid <- get_grid(shp, id_var = COWID, period_var = year)
```

The output is (by default) a list containing the raster object (`r`) used for creating the gridded data set along with the actual grid data set (`df`). To plot a particular variable, you can extract the raster object and assign the values for a particular variable to that object. See below: 

```{r plot grid, eval = TRUE}
# Extract raster
share_largest_count <- df_grid$r

# Assign values to grid cells
terra::values(share_largest_count) <- df_grid$data$polysh_largest_count

# Plot grid
ggplot() +
  geom_sf(data = afr) +
  tidyterra::geom_spatraster(data = share_largest_count, alpha = 0.7) +
  scale_fill_viridis_c(option = "plasma", direction = -1, na.value = NA) +
  labs(fill = "Share of polygons for the state with the largest \nnumber of polygons in total") +
  guides(fill = guide_colorbar(barwidth = 15)) +
  cappelenR::my_maptheme() +
  cappelenR::coord_bbox(afr) +
  theme(legend.position = "bottom", legend.title.position = "top",
        legend.justification = "center", legend.margin = margin(-2,0,0,0,unit = "cm"),
        legend.title = element_text(size = 12))
```


Again, it is also possible to create a panel version of the grid data, divided by specified period (by default it will create a panel for each 20-year period):

```{r grid panel, eval = TRUE, message = FALSE}
df_grid_panel <- get_grid(shp, id_var = COWID, by_period = TRUE, period_var = year)
```

```{r subset grid, eval = TRUE, echo = FALSE}
df_grid_panel$data <- df_grid_panel$data |>
  filter(stringr::str_detect(period, "^18")) |>
  filter(!stringr::str_detect(period, "1900$"))
```

Plotting the panel data requires creating a different layer for the raster object for each period and then assigning the values for that period. The code below provides a function for doing that. 

```{r plot panel grid, eval = TRUE}
# Define function for extracting raster grid and values for given variable
# This might be moved to its own proper function in the package
create_grid_panel <- function(x, var) {
  period_temp <- unique(x$data$period)
  r_temp <- x$r
  r_panel <- r_temp
  for (i in seq_along(period_temp[-1])) {
    terra::add(r_panel) <- r_temp
  }
  terra::set.names(r_panel, period_temp)
  for (i in period_temp) {
    terra::values(r_panel[[i]]) <- x$data[x$data$period == i, var]
  }
  return(r_panel)
}

share_largest_count_panel <- create_grid_panel(
  x = df_grid_panel, 
  var = "polysh_largest_count")

# Plot grid for each period
ggplot() +
  geom_sf(data = afr) +
  tidyterra::geom_spatraster(data = share_largest_count_panel, alpha = 0.7) +
  scale_fill_viridis_c(option = "plasma", direction = -1, na.value = NA) +
  labs(fill = "Share of polygons for the state with the largest \nnumber of polygons in total") +
  guides(fill = guide_colorbar(barwidth = 15)) +
  cappelenR::my_maptheme() +
  cappelenR::coord_bbox(afr) +
  theme(legend.position = "bottom", legend.title.position = "top",
        legend.justification = "center"
        # legend.margin = margin(-2,0,0,0,unit = "cm")
        ) +
  facet_wrap(~ lyr)
```
